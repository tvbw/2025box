# =================== æ™ºèƒ½ä¸‡èƒ½ CDN åŠ é€Ÿå—ï¼ˆ2025-12-08 æŒ‰å®æµ‹æ’åºï¼‰ ===================
import requests, time, threading, queue
from functools import lru_cache

sess = requests.Session()
adapter = requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=50)
sess.mount('http://', adapter)
sess.mount('https://', adapter)

# 1. æŒ‰ä½ æœ€æ–°æµ‹é€Ÿç»“æœæ’åºï¼ˆDNS 0 msï¼Œä¸‹è½½å‡å€¼ç”±ä½åˆ°é«˜ï¼‰
CDN_POOL = ['lib.baomitu.com', 'open.oppomobile.com']

# 2. æ­»é“¾æ›¿æ¢è¡¨ï¼ˆä¿æŒæœ€æ–°å¯ç”¨èŠ‚ç‚¹ï¼‰
DEAD_MAP = {
    'lib.baomitu.com':   CDN_POOL[0],
    'open.oppomobile.com': CDN_POOL[0]
}

# 3. å¹¶å‘æµ‹é€Ÿï¼ˆè¿”å› msï¼Œå¤±è´¥ 9999ï¼‰
def _ping(host):
    try:
        t0 = time.perf_counter()
        sess.head(f'https://{host}/favicon.ico', timeout=0.8)
        return int((time.perf_counter() - t0) * 1000)
    except:
        return 9999

# 4. å¯åŠ¨æ—¶è‡ªåŠ¨æ’åºï¼ˆåªè·‘ä¸€æ¬¡ï¼‰
def _rank_cdn():
    q = queue.Queue()
    for h in CDN_POOL:
        threading.Thread(target=lambda h: q.put((_ping(h), h)), args=(h,), daemon=True).start()
    ranked = sorted([q.get() for _ in CDN_POOL])
    return [h for _, h in ranked if _ < 9999] or CDN_POOL
FASTEST_CDN = _rank_cdn()
print('[CDN] è‡ªåŠ¨æµ‹é€Ÿå®Œæˆï¼Œæœ€ä¼˜é¡ºåºï¼š', FASTEST_CDN)

# 5. æ™ºèƒ½é€‰è·¯ + ç¼“å­˜ï¼ˆlru_cache çº¿ç¨‹å®‰å…¨ï¼‰
@lru_cache(maxsize=256)
def auto_cdn_url(url: str, follow_redirect: bool = False, proxy_base: str = None):
    if not url:
        return ''
    # 5.1 å…ˆæ›¿æ¢æ­»é“¾
    for dead, fast in DEAD_MAP.items():
        url = url.replace(dead, fast)
    # 5.2 è½®è¯¢æœ€ä¼˜ CDN
    host = url.split('/')[2]
    for cdn in FASTEST_CDN:
        if cdn == host:
            continue
        test = url.replace(host, cdn)
        try:
            r = sess.head(test, allow_redirects=False, timeout=2)
            if r.status_code == 200:
                url = test
                break
        except:
            continue
    # 5.3 302 è¿½è¸ªï¼ˆæ’­æ”¾é“¾éœ€è¦ï¼‰
    if follow_redirect:
        url = sess.get(url, stream=True, timeout=5).url
    # 5.4 ä»£ç†å‰ç¼€ï¼ˆå¯é€‰ï¼‰
    if proxy_base and not url.startswith(proxy_base):
        url = f'{proxy_base}{requests.utils.quote(url)}'
    return url
# =================== æ™ºèƒ½CDNå—ç»“æŸ ===================
import requests
from bs4 import BeautifulSoup
import re
from base.spider import Spider
import sys
import json
import base64
import urllib.parse
from Crypto.Cipher import ARC4
from Crypto.Util.Padding import unpad
import binascii

sys.path.append('..')

xurl = "https://www.fullhd.xxx/zh/"

headerx = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36'
          }

pm = ''

class Spider(Spider):
    global xurl
    global headerx

    def getName(self):
        return "é¦–é¡µ"

    def init(self, extend):
        pass

    def isVideoFormat(self, url):
        pass

    def manualVideoCheck(self):
        pass

    def extract_middle_text(self, text, start_str, end_str, pl, start_index1: str = '', end_index2: str = ''):
        if pl == 3:
            plx = []
            while True:
                start_index = text.find(start_str)
                if start_index == -1:
                    break
                end_index = text.find(end_str, start_index + len(start_str))
                if end_index == -1:
                    break
                middle_text = text[start_index + len(start_str):end_index]
                plx.append(middle_text)
                text = text.replace(start_str + middle_text + end_str, '')
            if len(plx) > 0:
                purl = ''
                for i in range(len(plx)):
                    matches = re.findall(start_index1, plx[i])
                    output = ""
                    for match in matches:
                        match3 = re.search(r'(?:^|[^0-9])(\d+)(?:[^0-9]|$)', match[1])
                        if match3:
                            number = match3.group(1)
                        else:
                            number = 0
                        if 'http' not in match[0]:
                            output += f"#{'ğŸ“½ï¸' + match[1]}${number}{xurl}{match[0]}"
                        else:
                            output += f"#{'ğŸ“½ï¸' + match[1]}${number}{match[0]}"
                    output = output[1:]
                    purl = purl + output + "$$$"
                purl = purl[:-3]
                return purl
            else:
                return ""
        else:
            start_index = text.find(start_str)
            if start_index == -1:
                return ""
            end_index = text.find(end_str, start_index + len(start_str))
            if end_index == -1:
                return ""

        if pl == 0:
            middle_text = text[start_index + len(start_str):end_index]
            return middle_text.replace("\\", "")

        if pl == 1:
            middle_text = text[start_index + len(start_str):end_index]
            matches = re.findall(start_index1, middle_text)
            if matches:
                jg = ' '.join(matches)
                return jg

        if pl == 2:
            middle_text = text[start_index + len(start_str):end_index]
            matches = re.findall(start_index1, middle_text)
            if matches:
                new_list = [f'âœ¨{item}' for item in matches]
                jg = '$$$'.join(new_list)
                return jg

    def homeContent(self, filter):
        result = {}
        result = {"class": [
        {"type_id": "latest-updates", "type_name": "æœ€æ–°è§†é¢‘ğŸŒ "},
        {"type_id": "top-rated", "type_name": "æœ€ä½³è§†é¢‘ğŸŒ "},
        {"type_id": "most-popular", "type_name": "çƒ­é—¨å½±ç‰‡ğŸŒ "},
        {"type_id": "networks/brazzers-com", "type_name": "BrazzersğŸŒ "},
        {"type_id": "networks/tushy-com", "type_name": "TushyğŸŒ "},
        {"type_id": "networks/naughtyamerica-com", "type_name": "NaughtyamericağŸŒ "},
        {"type_id": "sites/sexmex", "type_name": "SexmexğŸŒ "},
        {"type_id": "sites/passion-hd", "type_name": "Passion-HDğŸŒ "},
        {"type_id": "categories/animation", "type_name": "AnimationğŸŒ "},
        {"type_id": "categories/18-years-old", "type_name": "TeenğŸŒ "},
        {"type_id": "categories/pawg", "type_name": "PawgğŸŒ "},
        {"type_id": "categories/thong", "type_name": "ThongğŸŒ "},
        {"type_id": "categories/stockings", "type_name": "StockingsğŸŒ "},
        {"type_id": "categories/jav-uncensored", "type_name": "JAVğŸŒ "},
        {"type_id": "categories/pantyhose", "type_name": "PantyhoseğŸŒ "}
                            ],
                 }
        return result

    def homeVideoContent(self):
        videos = []
        try:
            detail = requests.get(url=xurl, headers=headerx)
            detail.encoding = "utf-8"
            res = detail.text
            doc = BeautifulSoup(res, "lxml")

            # Get videos from different sections
            sections = {
                "latest-updates": "æœ€æ–°è§†é¢‘",
                "top-rated": "æœ€ä½³è§†é¢‘",
                "most-popular": "çƒ­é—¨å½±ç‰‡"
            }
            
            for section_id, section_name in sections.items():
                section = doc.find('div', id=f"list_videos_videos_watched_right_now_items")
                if not section:
                    continue
                    
                vods = section.find_all('div', class_="item")
                for vod in vods:
                    names = vod.find_all('a')
                    name = names[0]['title'] if names and 'title' in names[0].attrs else section_name

                    ids = vod.find_all('a')
                    id = ids[0]['href'] if ids else ""

                    # è·å–å›¾ç‰‡ - é€‚é…ä¸¤ç§ä¸åŒçš„imgæ ‡ç­¾ç»“æ„
                    pic = ""
                    # ç¬¬ä¸€ç§æ–¹å¼ï¼šæŸ¥æ‰¾å¸¦æœ‰data-srcå±æ€§çš„imgæ ‡ç­¾
                    pics = vod.find('img', class_="lazyload")
                    if pics and pics.get('data-src'):
                        pic = pics['data-src']
                    # ç¬¬äºŒç§æ–¹å¼ï¼šæŸ¥æ‰¾å¸¦æœ‰srcå±æ€§çš„imgæ ‡ç­¾
                    if not pic:
                        pics = vod.find('img', class_="thumb_img")
                        if pics and pics.get('src'):
                            pic = pics['src']
                    
                    # å¦‚æœæ‰¾åˆ°äº†å›¾ç‰‡ä½†URLä¸å®Œæ•´ï¼Œæ·»åŠ åŸºç¡€URL
                    if pic and 'http' not in pic:
                        pic = xurl + pic

                    remarks = vod.find('span', class_="duration")
                    remark = remarks.text.strip() if remarks else ""

                    video = {
                        "vod_id": id,
                        "vod_name": name,
                        "vod_pic": pic,
                        "vod_remarks": remark
                    }
                    videos.append(video)

            result = {'list': videos}
            return result
        except Exception as e:
            print(f"Error in homeVideoContent: {str(e)}")
            return {'list': []}

    def categoryContent(self, cid, pg, filter, ext):
        result = {}
        videos = []
        try:
            if pg and int(pg) > 1:
                url = f'{xurl}/{cid}/{pg}/'
            else:
                url = f'{xurl}/{cid}/'

            detail = requests.get(url=url, headers=headerx)
            detail.encoding = "utf-8"
            res = detail.text
            doc = BeautifulSoup(res, "lxml")

            section = doc.find('div', class_="list-videos")
            if section:
                vods = section.find_all('div', class_="item")
                for vod in vods:
                    names = vod.find_all('a')
                    name = names[0]['title'] if names and 'title' in names[0].attrs else ""

                    ids = vod.find_all('a')
                    id = ids[0]['href'] if ids else ""

                    # è·å–å›¾ç‰‡ - é€‚é…ä¸¤ç§ä¸åŒçš„imgæ ‡ç­¾ç»“æ„
                    pic = ""
                    # ç¬¬ä¸€ç§æ–¹å¼ï¼šæŸ¥æ‰¾å¸¦æœ‰data-srcå±æ€§çš„imgæ ‡ç­¾
                    pics = vod.find('img', class_="lazyload")
                    if pics and pics.get('data-src'):
                        pic = pics['data-src']
                    # ç¬¬äºŒç§æ–¹å¼ï¼šæŸ¥æ‰¾å¸¦æœ‰srcå±æ€§çš„imgæ ‡ç­¾
                    if not pic:
                        pics = vod.find('img', class_="thumb_img")
                        if pics and pics.get('src'):
                            pic = pics['src']
                    
                    # å¦‚æœæ‰¾åˆ°äº†å›¾ç‰‡ä½†URLä¸å®Œæ•´ï¼Œæ·»åŠ åŸºç¡€URL
                    if pic and 'http' not in pic:
                        pic = xurl + pic

                    remarks = vod.find('span', class_="duration")
                    remark = remarks.text.strip() if remarks else ""

                    video = {
                        "vod_id": id,
                        "vod_name": name,
                        "vod_pic": pic,
                        "vod_remarks": remark
                    }
                    videos.append(video)

        except Exception as e:
            print(f"Error in categoryContent: {str(e)}")

        result = {
            'list': videos,
            'page': pg,
            'pagecount': 9999,
            'limit': 90,
            'total': 999999
        }
        return result

    def detailContent(self, ids):
        global pm
        did = ids[0]
        result = {}
        videos = []
        playurl = ''
        if 'http' not in did:
            did = xurl + did
        res1 = requests.get(url=did, headers=headerx)
        res1.encoding = "utf-8"
        res = res1.text

        content = 'ğŸ‘‰' + self.extract_middle_text(res,'<h1>','</h1>', 0)

        yanuan = self.extract_middle_text(res, '<span>Pornstars:</span>','</div>',1, 'href=".*?">(.*?)</a>')

        bofang = did

        videos.append({
            "vod_id": did,
            "vod_actor": yanuan,
            "vod_director": '',
            "vod_content": content,
            "vod_play_from": 'ä¹¦ç”Ÿç©å‰£â±Â·*â‚ï¼‡',
            "vod_play_url": bofang
                     })

        result['list'] = videos
        return result

    def playerContent(self, flag, id, vipFlags):
        parts = id.split("http")
        xiutan = 0
        if xiutan == 0:
            if len(parts) > 1:
                before_https, after_https = parts[0], 'http' + parts[1]
            res = requests.get(url=after_https, headers=headerx)
            res = res.text

            url2 = self.extract_middle_text(res, '<video', '</video>', 0).replace('\\', '')
            soup = BeautifulSoup(url2, 'html.parser')
            first_source = soup.find('source')
            src_value = first_source.get('src')

            response = requests.head(src_value, allow_redirects=False)
            if response.status_code == 302:
                redirect_url = response.headers['Location']

            response = requests.head(redirect_url, allow_redirects=False)
            if response.status_code == 302:
                redirect_url = response.headers['Location']

            result = {}
            result["parse"] = xiutan
            result["playUrl"] = ''
            result["url"] = redirect_url
            result["header"] = headerx
            return result

    def searchContentPage(self, key, quick, page):
        result = {}
        videos = []
        if not page:
            page = '1'
        if page == '1':
            url = f'{xurl}/search/{key}/'
        else:
            url = f'{xurl}/search/{key}/{str(page)}/'

        try:
            detail = requests.get(url=url, headers=headerx)
            detail.encoding = "utf-8"
            res = detail.text
            doc = BeautifulSoup(res, "lxml")

            section = doc.find('div', class_="list-videos")
            if section:
                vods = section.find_all('div', class_="item")
                for vod in vods:
                    names = vod.find_all('a')
                    name = names[0]['title'] if names and 'title' in names[0].attrs else ""

                    ids = vod.find_all('a')
                    id = ids[0]['href'] if ids else ""

                    # è·å–å›¾ç‰‡ - é€‚é…ä¸¤ç§ä¸åŒçš„imgæ ‡ç­¾ç»“æ„
                    pic = ""
                    # ç¬¬ä¸€ç§æ–¹å¼ï¼šæŸ¥æ‰¾å¸¦æœ‰data-srcå±æ€§çš„imgæ ‡ç­¾
                    pics = vod.find('img', class_="lazyload")
                    if pics and pics.get('data-src'):
                        pic = pics['data-src']
                    # ç¬¬äºŒç§æ–¹å¼ï¼šæŸ¥æ‰¾å¸¦æœ‰srcå±æ€§çš„imgæ ‡ç­¾
                    if not pic:
                        pics = vod.find('img', class_="thumb_img")
                        if pics and pics.get('src'):
                            pic = pics['src']
                    
                    # å¦‚æœæ‰¾åˆ°äº†å›¾ç‰‡ä½†URLä¸å®Œæ•´ï¼Œæ·»åŠ åŸºç¡€URL
                    if pic and 'http' not in pic:
                        pic = xurl + pic

                    remarks = vod.find('span', class_="duration")
                    remark = remarks.text.strip() if remarks else ""

                    video = {
                        "vod_id": id,
                        "vod_name": name,
                        "vod_pic": pic,
                        "vod_remarks": remark
                    }
                    videos.append(video)
        except Exception as e:
            print(f"Error in searchContentPage: {str(e)}")

        result = {
            'list': videos,
            'page': page,
            'pagecount': 9999,
            'limit': 90,
            'total': 999999
        }
        return result

    def searchContent(self, key, quick):
        return self.searchContentPage(key, quick, '1')

    def localProxy(self, params):
        if params['type'] == "m3u8":
            return self.proxyM3u8(params)
        elif params['type'] == "media":
            return self.proxyMedia(params)
        elif params['type'] == "ts":
            return self.proxyTs(params)

        return None
